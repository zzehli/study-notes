{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b453c53",
   "metadata": {},
   "source": [
    "Like SGD, Adam is a optimization algorithm to locate the minium of a function. It can be understood as a combination of SGD with **momentum** and **RMSProp**. Momentum uses a moving average instead of the previous weight to perform weight update. RMSProp assign learning rate to individual parameters based on a moving rage of squared weights. Adams combines the two.\n",
    "\n",
    "Similar to the one performed in SGD, this entry will follow a similar pattern to implement these optimization algorithms by approximating a quadratic form.\n",
    "\n",
    "TODO: link sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5782d",
   "metadata": {},
   "source": [
    "We prep the data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f9db90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.1243,  6.9424,  9.1506])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# mean square error\n",
    "def mse(preds, acts):\n",
    "    return ((preds - acts) ** 2).mean()\n",
    "\n",
    "\n",
    "def quad(a, b, c, x):\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "\n",
    "def mk_quad(a, b, c):\n",
    "    return partial(quad, a, b, c)\n",
    "\n",
    "\n",
    "# target model\n",
    "f = mk_quad(2, 3, 4)\n",
    "f(2)\n",
    "\n",
    "# assume some data points\n",
    "x = torch.linspace(-2, 2, 20)[:, None]\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate a tensor of random numbers with the same shape as f(x)\n",
    "# torch.rand_like(f(x)) generates random numbers between 0 and 1\n",
    "# with the same shape as f(x). We scale and shift it to the desired range.\n",
    "random_numbers = torch.rand_like(f(x)) * 10 - 5\n",
    "\n",
    "# dataset\n",
    "y = f(x) + random_numbers\n",
    "\n",
    "\n",
    "# loss function\n",
    "def quad_mse(params):\n",
    "    f = mk_quad(*params)\n",
    "    return mse(f(x), y)\n",
    "\n",
    "\n",
    "# initial params\n",
    "params = torch.tensor([4, 5.0, 7.0])\n",
    "params.requires_grad_()\n",
    "\n",
    "loss = quad_mse(params)\n",
    "loss\n",
    "\n",
    "loss.backward()\n",
    "params.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7548f7e",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114a1b5",
   "metadata": {},
   "source": [
    "\n",
    "Momentum allow the weight update to gain momentum, a moving inertia based on moving averages. It allows the weight update to overcome small variations. We introduce a parameter $\\beta$ to denote how much momentum to use. If $\\beta$ is 0, then the weight is not affected by the moving average. The algorithm for momentum is the following:\n",
    "```\n",
    "weight.avg = beta * weight.avg + (1-beta) * weight.grad\n",
    "new_weight = weight - lr * weight.avg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1d63557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 41.75225067138672\n",
      "loss 22.198631286621094\n",
      "loss 13.591499328613281\n",
      "loss 10.369937896728516\n",
      "loss 8.952920913696289\n",
      "loss 8.165651321411133\n",
      "loss 7.653504848480225\n",
      "loss 7.295266628265381\n",
      "loss 7.037405490875244\n",
      "loss 6.849569797515869\n"
     ]
    }
   ],
   "source": [
    "beta = 0.1\n",
    "lr = 0.05\n",
    "params = torch.tensor([4, 5.0, 7.0])\n",
    "params.requires_grad_()\n",
    "weight_avg = torch.zeros(params.shape)\n",
    "for _ in range(10):\n",
    "    loss = quad_mse(params)\n",
    "    print(\"loss\", loss.item())\n",
    "    loss.backward()\n",
    "    weight_avg = beta * weight_avg + (1 - beta) * params.grad.data\n",
    "    params.data -= lr * weight_avg\n",
    "    params.grad = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3afc3",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471cf3a",
   "metadata": {},
   "source": [
    "RMSProp allow each parameter gets its own specific learning rate controlled by a global learning rate. We determine the tendency of the learning rate using a moving average of squared weights. Instead of simple average, we use squared average because we want to capture the magnitude of the change. We introduce $\\alpha$, which serves the same purpose as $\\beta$ in momentum. The $\\epsilon$ is for numerical stability.\n",
    "\n",
    "For RMSProp, we implement the following algorithm:\n",
    "```\n",
    "w.square_avg = alpha * w.square_avg + (1-alpha) * (w.grad ** 2)\n",
    "new_w = w - lr * w.grad / math.sqrt(w.square_avg + eps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c7852df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 41.75225067138672\n",
      "loss 25.9729061126709\n",
      "loss 18.755748748779297\n",
      "loss 14.587450981140137\n",
      "loss 11.957747459411621\n",
      "loss 10.221823692321777\n",
      "loss 9.04482650756836\n",
      "loss 8.232916831970215\n",
      "loss 7.666159152984619\n",
      "loss 7.267086982727051\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.99\n",
    "eps = 1e-8\n",
    "lr = 0.05\n",
    "params_rp = torch.tensor([4, 5.0, 7.0])\n",
    "params_rp.requires_grad_()\n",
    "sqr_avg = torch.zeros(params_rp.shape)\n",
    "for _ in range(10):\n",
    "    loss_rp = quad_mse(params_rp)\n",
    "    print(\"loss\", loss_rp.item())\n",
    "    loss_rp.backward()\n",
    "    sqr_avg = alpha * sqr_avg + (1 - alpha) * (params_rp.grad.data**2)\n",
    "    params_rp.data -= lr * params_rp.grad.data / torch.sqrt(sqr_avg + eps)\n",
    "    params_rp.grad = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0ac0c",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e72c3",
   "metadata": {},
   "source": [
    "We combine momentum and RMSProp and get Adam. Unlike momentum however, Adam uses *unbiased* average. For adam, we implement the following algorithm:\n",
    "```\n",
    "w.avg = beta1 * w.avg + (1-beta1) * w.grad\n",
    "unbias_avg = w.avg / (1 - (beta1**(i+1)))\n",
    "w.sqr_avg = beta2 * w.sqr_avg + (1-beta2) * (w.grad ** 2)\n",
    "new_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "709f0f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 41.75225067138672\n",
      "loss 25.913122177124023\n",
      "loss 18.47113609313965\n",
      "loss 14.244725227355957\n",
      "loss 11.626660346984863\n",
      "loss 9.926828384399414\n",
      "loss 8.792333602905273\n",
      "loss 8.021905899047852\n",
      "loss 7.492585182189941\n",
      "loss 7.125898838043213\n"
     ]
    }
   ],
   "source": [
    "beta = 0.1\n",
    "alpha = 0.99\n",
    "eps = 1e-8\n",
    "lr = 0.05\n",
    "wd = 0.01  # decoupled weight decay\n",
    "params_ad = torch.tensor([4, 5.0, 7.0])\n",
    "params_ad.requires_grad_()\n",
    "sqr_avg = torch.zeros(params_ad.shape)\n",
    "weight_avg = torch.zeros(params_ad.shape)\n",
    "for i in range(10):\n",
    "    loss_rp = quad_mse(params_ad)\n",
    "    print(\"loss\", loss_rp.item())\n",
    "    loss_rp.backward()\n",
    "    weight_avg = beta * weight_avg + (1 - beta) * params_ad.grad.data\n",
    "    unbiased_avg = weight_avg / (1 - (beta ** (i + 1)))\n",
    "    sqr_avg = alpha * sqr_avg + (1 - alpha) * (params_ad.grad.data**2)\n",
    "    params_ad.data -= lr * unbiased_avg / torch.sqrt(sqr_avg + eps)\n",
    "    params_ad.data -= lr * params_ad.data * wd\n",
    "    params_ad.grad = None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
