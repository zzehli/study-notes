{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103fe804",
   "metadata": {},
   "source": [
    "Weight decay, or L2 regularization, is a regularization method where the sum of all weights squared with a weight decay factor is add to the loss function. The intuition behind weight decay is the following. We add a term with weight params to the loss function. As a result, the gradient of the loss function will be larger, so the parameters will descend more quickly proportional to the size of the weight. This will move the parameters to smaller values.\n",
    "\n",
    "That is we add an extra term to the loss function, where $w_i$ are weights and $\\lambda$ is the weight decay factor:\n",
    "$$\n",
    "L_{total} = L_{original} + \\lambda \\sum_i w_i^2 \n",
    "$$\n",
    "When weights are large, the $\\lambda \\sum_i w_i^2$ term grows and leads to larger gradients and therefore quicker descend. \n",
    "\n",
    "The original SGD becomes:\n",
    "$$\n",
    "\\frac{\\partial L_{total}}{\\partial w} = \\frac{\\partial L_{original}}{\\partial w} + 2\\lambda w\n",
    "$$\n",
    "In practice, we ignore the coefficient 2 in front of the weight decay term.\n",
    "\n",
    "TODO: add highlight note\n",
    "\n",
    "So the update rule becomes\n",
    "$$\n",
    "w_{new} \\leftarrow w_{original} - \\eta \\left(\\frac{\\partial L_{original}}{\\partial w_{original}} + 2\\lambda w_{original} \\right)\n",
    "$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "\n",
    "TODO: add SGD link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51303c4",
   "metadata": {},
   "source": [
    "In the following example, we illustrate how weight decay works by showing how weight decay works for function $f = x^2$, we show side by side how SGD steps with and without a weight decay factor with two optimizers. As we can see, the resulting weights with weight decay factor ended up smaller than the weights without weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb7a12ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight: 1.0\n",
      "After step without weight decay: 0.800000011920929\n",
      "After step with weight decay: 0.7900000214576721\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "opt_no_wd = torch.optim.SGD([w], lr=0.1, weight_decay=0.0)\n",
    "loss = w * w\n",
    "print(\"Initial weight:\", w.item())\n",
    "\n",
    "opt_no_wd.zero_grad()\n",
    "loss.backward()\n",
    "opt_no_wd.step()\n",
    "print(\"After step without weight decay:\", w.item())\n",
    "\n",
    "# Reset weight\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "opt_wd = torch.optim.SGD([w], lr=0.1, weight_decay=0.1)\n",
    "\n",
    "# Step with weight decay\n",
    "opt_wd.zero_grad()\n",
    "loss = w * w\n",
    "loss.backward()\n",
    "opt_wd.step()\n",
    "print(\"After step with weight decay:\", w.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
