{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3eb723",
   "metadata": {},
   "source": [
    "Recurrent Neural Net (RNN) uses recursion as a strategy to create deep neural nets. RNNs use the same hidden weight matrix for all additional layers, which make it memory efficient. It trains this weight matrix by recursively incorporating the next token in the training sequence (like a sentence). RNN is best suited for sequential data like natural language or time series.\n",
    "\n",
    "Suppose the hidden layer is $W$, $e$ are token embeddings, and $h_o$ is the output layer:\n",
    "\n",
    "The first hidden state/activation is $$h_0 = \\text{ReLU}\\big(W\\,e_0 + b\\big)$$\n",
    "The second hidden state is $$h_1 = \\text{ReLU}\\big(W\\,(h_0 + e_1) + b\\big)$$\n",
    "The third hidden state: $$h_2 = \\text{ReLU}\\big(W\\,(h_1 + e_2) + b\\big)$$\n",
    "The output predictions: $$\\text{logits} = h_o(h_2)$$\n",
    "Notice that throughout this process, the hidden layer, $W$, stays the same in the forward pass.\n",
    "\n",
    "In general, the standard RNN is formulated as\n",
    "$$\n",
    "h_t = f(W h_{t-1} + U e_t + b)\n",
    "$$\n",
    "\n",
    "* $h_{t-1}$ : previous hidden state\n",
    "* $e_t$ : embedding (input at this step)\n",
    "* $W$ : hidden-to-hidden weights (processes the past)\n",
    "* $U$ : input-to-hidden weights (processes the current token)\n",
    "* $b$ : bias\n",
    "* $f$ :the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271919be",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e86c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://s3.amazonaws.com/fast-ai-sample/human_numbers.tgz\" -O \"../data/human_numbers.tgz\" && tar -xzf \"../data/human_numbers.tgz\" -C ../data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab888ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70aa059d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('../data/human_numbers/train.txt'), PosixPath('../data/human_numbers/valid.txt')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "sample_path = Path(\"../data/human_numbers\")\n",
    "print(list(sample_path.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae013b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9998"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = []\n",
    "with open(sample_path / \"train.txt\") as f:\n",
    "    lines += [*f.readlines()]\n",
    "with open(sample_path / \"valid.txt\") as f:\n",
    "    lines += [*f.readlines()]\n",
    "len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa79a27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,\n",
       " ['nine',\n",
       "  'fifty',\n",
       "  'one',\n",
       "  'thirty',\n",
       "  'hundred',\n",
       "  'two',\n",
       "  '.',\n",
       "  'fifteen',\n",
       "  'thousand',\n",
       "  'seven'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" . \".join([l.strip() for l in lines])\n",
    "tokens = text.split(\" \")\n",
    "vocab = set(tokens)\n",
    "len(vocab), list(vocab)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73fe950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63095"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "nums = [word2idx[i] for i in tokens]\n",
    "len(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb35b27",
   "metadata": {},
   "source": [
    "Our neural network will predict next word in a sequence. The training data will have three words and the model will predict the forth word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53859495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ad71f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['one', '.', 'two'], '.'),\n",
       " (['.', 'three', '.'], 'four'),\n",
       " (['four', '.', 'five'], '.'),\n",
       " (['.', 'six', '.'], 'seven'),\n",
       " (['seven', '.', 'eight'], '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tokens[i : i + 3], tokens[i + 3]) for i in range(0, len(tokens) - 4, 3)][:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb178b",
   "metadata": {},
   "source": [
    "convert that into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313e004d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([2, 6, 5]), 6),\n",
       " (tensor([ 6, 16,  6]), 27),\n",
       " (tensor([27,  6, 29]), 6),\n",
       " (tensor([ 6, 18,  6]), 9),\n",
       " (tensor([ 9,  6, 14]), 6)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = [\n",
    "    (torch.tensor(nums[i : i + 3]), nums[i + 3]) for i in range(0, len(nums) - 4, 3)\n",
    "]\n",
    "seqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5368828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls_train = DataLoader(seqs[:cut], batch_size=bs)\n",
    "dls_valid = DataLoader(seqs[cut:], batch_size=bs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8016f1",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b6b8c",
   "metadata": {},
   "source": [
    "A literal interpretation of RNN where the same hidden layer is applied to the embeddings one at a time. But the hidden layer is the same at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23abd414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel1(nn.Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        # hidden layer\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        # output layer\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h is hidden activation\n",
    "        h = F.relu(self.h_h(self.i_h(x[:, 0])))\n",
    "        h = h + self.i_h(x[:, 1])\n",
    "        # use the same hidden layer for all input\n",
    "        h = F.relu(self.h_h(h))\n",
    "        h = h + self.i_h(x[:, 2])\n",
    "        h = F.relu(self.h_h(h))\n",
    "        return self.h_o(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c556e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted index: 26\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(dls_train))\n",
    "rnn1 = LMModel1(len(vocab), 3)\n",
    "outputs = rnn1(xb)\n",
    "# take the index that gives the biggest elements in outputs tensor\n",
    "pred_idx = outputs.argmax(dim=-1)\n",
    "print(\"Predicted index:\", pred_idx.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53b9b9",
   "metadata": {},
   "source": [
    "Rewrite the forward pass with loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de509991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2(nn.Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        # hidden layer\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        # output layer\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = 0\n",
    "        for i in range(3):\n",
    "            h = h + self.i_h(x[:, i])\n",
    "            # use the same hidden layer for all input\n",
    "            h = F.relu(self.h_h(h))\n",
    "        return self.h_o(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19606f2",
   "metadata": {},
   "source": [
    "Make the hidden state persists over each step by making it a class attribute. Since the hidden state exists forever, keeping track and calculating its gradient become very expensive as the nn becomes very deep. We will use `detach()` to remove gradient history, and keep only the most recent 3 gradients instead.\n",
    "\n",
    "To make this work, we also need to change our `DataLoader` to generate continuous sequences across batch. That's to say, if we have batch size `bs`, our dataset is divided into `m = len(dset) // bs` groups (the # of batches). Across these groups, sequence at index `i` should follow one another. That is to say, ith sequence in every batch should follow one other.\n",
    "\n",
    "Say our batch size is 3, we will have 7010 groups, our original dataset is `seqs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d72771a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7010, 3, 21031)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 3\n",
    "m = len(seqs) // bs\n",
    "m, bs, len(seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0632894b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([2, 6, 5]), 6), (tensor([ 6, 16,  6]), 27), (tensor([27,  6, 29]), 6)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a42bc",
   "metadata": {},
   "source": [
    "We define `group_chunks` to load our dataset based on the logic above. As we can see, the resulting batches `bx, cx, dx` have their 1st element the same as the first 3 elements of `seqs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f49a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2,  6,  5],\n",
       "         [ 8, 14,  4],\n",
       "         [29,  8, 27]]),\n",
       " tensor([[ 6, 16,  6],\n",
       "         [25, 29,  6],\n",
       "         [ 4, 25,  5]]),\n",
       " tensor([[27,  6, 29],\n",
       "         [ 5,  8, 14],\n",
       "         [ 6, 29,  8]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = []\n",
    "    for i in range(m):\n",
    "        new_ds += [ds[i + m * j] for j in range(bs)]\n",
    "    return new_ds\n",
    "\n",
    "\n",
    "dls_train = DataLoader(group_chunks(seqs[:cut], bs), batch_size=bs, drop_last=True)\n",
    "it = iter(dls_train)\n",
    "bx, by = next(it)\n",
    "cx, cy = next(it)\n",
    "dx, dy = next(it)\n",
    "bx[:10], cx[:10], dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a518e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel3(nn.Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        # hidden layer\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        # output layer\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        # initiate hidden state\n",
    "        self.h = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(3):\n",
    "            self.h = self.h + self.i_h(x[:, i])\n",
    "            # use the same hidden layer for all input\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "        out = self.h_o(self.h)\n",
    "        # remove old gradient\n",
    "        self.h = self.h.detach()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8220417",
   "metadata": {},
   "source": [
    "Next, instead of predict every 3 words, we predict every word. Todo so, we modify our dataset further, since `seqs` is defined for every 3 words. We define the sequence length as `sl`, each element in `seqs` now contains 2 element, offset by 1 index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7976ab48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([ 2,  6,  5,  6, 16,  6, 27,  6, 29,  6, 18,  6,  9,  6, 14,  6]),\n",
       "  tensor([ 6,  5,  6, 16,  6, 27,  6, 29,  6, 18,  6,  9,  6, 14,  6,  0])),\n",
       " (tensor([ 0,  6, 17,  6, 13,  6, 10,  6, 20,  6, 26,  6,  7,  6, 19,  6]),\n",
       "  tensor([ 6, 17,  6, 13,  6, 10,  6, 20,  6, 26,  6,  7,  6, 19,  6, 21])),\n",
       " (tensor([21,  6, 23,  6, 22,  6, 12,  6, 12,  2,  6, 12,  5,  6, 12, 16]),\n",
       "  tensor([ 6, 23,  6, 22,  6, 12,  6, 12,  2,  6, 12,  5,  6, 12, 16,  6])),\n",
       " (tensor([ 6, 12, 27,  6, 12, 29,  6, 12, 18,  6, 12,  9,  6, 12, 14,  6]),\n",
       "  tensor([12, 27,  6, 12, 29,  6, 12, 18,  6, 12,  9,  6, 12, 14,  6, 12])),\n",
       " (tensor([12,  0,  6,  3,  6,  3,  2,  6,  3,  5,  6,  3, 16,  6,  3, 27]),\n",
       "  tensor([ 0,  6,  3,  6,  3,  2,  6,  3,  5,  6,  3, 16,  6,  3, 27,  6]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = 16\n",
    "seqs = [\n",
    "    (torch.tensor(nums[i : i + sl]), torch.tensor(nums[i + 1 : i + sl + 1]))\n",
    "    for i in range(0, len(nums) - sl - 1, sl)\n",
    "]\n",
    "cut = int(len(seqs) * 0.8)\n",
    "seqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f4919b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel4(nn.Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, bs):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        # hidden layer\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        # output layer\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        # store dimensions for proper initialization\n",
    "        self.n_hidden = n_hidden\n",
    "        # initiate hidden state properly\n",
    "        self.h = torch.zeros(bs, n_hidden)\n",
    "        # self.h = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, sl = x.shape\n",
    "\n",
    "        out = []\n",
    "        for i in range(sl):\n",
    "            # Add embedding to hidden state\n",
    "            self.h = self.h + self.i_h(x[:, i])\n",
    "            # Apply hidden layer with activation\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "            # Generate output for this timestep\n",
    "            out.append(self.h_o(self.h))\n",
    "        \n",
    "        # Detach hidden state to prevent gradient explosion\n",
    "        self.h = self.h.detach()\n",
    "        return torch.stack(out, dim=1)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the hidden state\"\"\"\n",
    "        self.h = torch.zeros(bs, self.n_hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482c7c2",
   "metadata": {},
   "source": [
    "The output shape of the model is `bs x sl x vocab_sz`, our valid data are `bs x sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6766493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 16, 30]), torch.Size([64, 16]), torch.Size([64, 16]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "dls_train = DataLoader(group_chunks(seqs[:cut], bs), batch_size=bs, drop_last=True)\n",
    "dls_valid = DataLoader(group_chunks(seqs[cut:], bs), batch_size=bs, drop_last=True)\n",
    "\n",
    "xb, yb = next(iter(dls_train))\n",
    "rnn2 = LMModel4(len(vocab), 64, bs)\n",
    "rnn2(xb).shape, xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5030a",
   "metadata": {},
   "source": [
    "We make the align by flattening them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb752662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4675, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2(xb).view(-1, len(vocab)).shape, yb.view(-1).shape\n",
    "F.cross_entropy(rnn2(xb).view(-1, len(vocab)), yb.view(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acdbc9",
   "metadata": {},
   "source": [
    "Based on the above, we define our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a7e3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(inp, targ):\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c674faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(pred, target):\n",
    "    # pred: (bs, sl, vocab), targ: (bs, sl)\n",
    "    return (pred.argmax(-1) == target).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c3309f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 49\n",
      "Validation batches: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training batches: {len(dls_train)}\")\n",
    "print(f\"Validation batches: {len(dls_valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41673ea",
   "metadata": {},
   "source": [
    "We reset the hidden state of the model at the beginning of each train and validation phases of an epoch. this will make sure we start with a clean state before reading those continuous chunks of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e3299ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "# Use a lower learning rate to start\n",
    "rnn2 = LMModel4(len(vocab), 64, bs)\n",
    "\n",
    "optimizer = torch.optim.SGD(rnn2.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.01, steps_per_epoch=len(dls_train), epochs=epochs\n",
    ")\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.benchmark = True  # good if input sizes are consistent\n",
    "def train(model, epochs, train_loader, valid_loader):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = torch.zeros(())\n",
    "        batch_num = 0\n",
    "        model.train()\n",
    "        model.reset() # reset hidden state at the beginning of each epoch\n",
    "        for xb, yb in train_loader:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_num += 1  # number of batches within a epoch\n",
    "        avg_loss = epoch_loss / batch_num\n",
    "        model.eval()\n",
    "        model.reset()\n",
    "        with torch.no_grad():\n",
    "            batch_num_valid = 0\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            for xb, yb in valid_loader:\n",
    "                pred = model(xb)\n",
    "                valid_loss += loss_func(pred, yb).item()\n",
    "                valid_acc += batch_accuracy(pred, yb)\n",
    "                batch_num_valid += 1\n",
    "        print(f\"epoch {epoch}, train loss: {avg_loss:.4f}\")\n",
    "        print(f\"validation loss {valid_loss / batch_num_valid:.4f}\")\n",
    "        print(f\"validation accuracy {valid_acc / batch_num_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3fd07b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss: 3.3567\n",
      "validation loss 3.2219\n",
      "validation accuracy 0.1353\n",
      "epoch 1, train loss: 2.8462\n",
      "validation loss 2.5005\n",
      "validation accuracy 0.4107\n",
      "epoch 2, train loss: 1.9836\n",
      "validation loss 2.0847\n",
      "validation accuracy 0.4659\n",
      "epoch 3, train loss: 1.5911\n",
      "validation loss 1.8851\n",
      "validation accuracy 0.4338\n",
      "epoch 4, train loss: 1.5050\n",
      "validation loss 1.9142\n",
      "validation accuracy 0.3676\n",
      "epoch 5, train loss: 1.4625\n",
      "validation loss 1.9384\n",
      "validation accuracy 0.3444\n",
      "epoch 6, train loss: 1.4268\n",
      "validation loss 1.9259\n",
      "validation accuracy 0.3503\n",
      "epoch 7, train loss: 1.3876\n",
      "validation loss 1.8492\n",
      "validation accuracy 0.3971\n",
      "epoch 8, train loss: 1.3426\n",
      "validation loss 1.7483\n",
      "validation accuracy 0.4548\n",
      "epoch 9, train loss: 1.3006\n",
      "validation loss 1.7162\n",
      "validation accuracy 0.4701\n",
      "epoch 10, train loss: 1.2694\n",
      "validation loss 1.6989\n",
      "validation accuracy 0.4807\n",
      "epoch 11, train loss: 1.2361\n",
      "validation loss 1.7161\n",
      "validation accuracy 0.4776\n",
      "epoch 12, train loss: 1.1958\n",
      "validation loss 1.7573\n",
      "validation accuracy 0.4581\n",
      "epoch 13, train loss: 1.1628\n",
      "validation loss 1.7708\n",
      "validation accuracy 0.4549\n",
      "epoch 14, train loss: 1.1491\n",
      "validation loss 1.5593\n",
      "validation accuracy 0.5046\n",
      "epoch 15, train loss: 1.0883\n",
      "validation loss 1.5321\n",
      "validation accuracy 0.5219\n",
      "epoch 16, train loss: 1.0617\n",
      "validation loss 1.5656\n",
      "validation accuracy 0.5221\n",
      "epoch 17, train loss: 1.0489\n",
      "validation loss 1.5489\n",
      "validation accuracy 0.5273\n",
      "epoch 18, train loss: 1.0331\n",
      "validation loss 1.5737\n",
      "validation accuracy 0.5226\n",
      "epoch 19, train loss: 1.0211\n",
      "validation loss 1.5607\n",
      "validation accuracy 0.5251\n"
     ]
    }
   ],
   "source": [
    "train(rnn2, epochs, dls_train, dls_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1421e5c",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM)\n",
    "\n",
    "Next, we will create LSTM, which solves the problem of vanishing/exploding gradients. The LSTM cell includes two hidden states instead of one in classic RNN. In classic RNN, the hidden state is responsible for:\n",
    "    1. Having the right information for the output layer to predict the correct next token\n",
    "    2. Retaining memory of everything that happened in the sentence\n",
    "It turns out that RNN is bad at memorizing things distant in the memory. So we introduces a **cell state** to keep track of the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.forget_gate = nn.Linear(ni + nh, nh)\n",
    "        self.input_gate  = nn.Linear(ni + nh, nh)\n",
    "        self.cell_gate   = nn.Linear(ni + nh, nh)\n",
    "        self.output_gate = nn.Linear(ni + nh, nh)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        h,c = state\n",
    "        h = torch.cat([h, input], dim=1)\n",
    "        forget = torch.sigmoid(self.forget_gate(h))\n",
    "        c = c * forget\n",
    "        inp = torch.sigmoid(self.input_gate(h))\n",
    "        cell = torch.tanh(self.cell_gate(h))\n",
    "        c = c + inp * cell\n",
    "        out = torch.sigmoid(self.output_gate(h))\n",
    "        h = out * torch.tanh(c)\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01517364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel5(nn.Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        super().__init__()\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0ce1e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss: 3.3705\n",
      "validation loss 3.3514\n",
      "validation accuracy 0.1423\n",
      "epoch 1, train loss: 3.3021\n",
      "validation loss 3.2490\n",
      "validation accuracy 0.1422\n",
      "epoch 2, train loss: 3.1584\n",
      "validation loss 3.0872\n",
      "validation accuracy 0.1580\n",
      "epoch 3, train loss: 2.9764\n",
      "validation loss 2.9400\n",
      "validation accuracy 0.1692\n",
      "epoch 4, train loss: 2.8543\n",
      "validation loss 2.8835\n",
      "validation accuracy 0.1530\n",
      "epoch 5, train loss: 2.7984\n",
      "validation loss 2.8554\n",
      "validation accuracy 0.1645\n",
      "epoch 6, train loss: 2.7654\n",
      "validation loss 2.8339\n",
      "validation accuracy 0.1996\n",
      "epoch 7, train loss: 2.7436\n",
      "validation loss 2.8132\n",
      "validation accuracy 0.2486\n",
      "epoch 8, train loss: 2.7256\n",
      "validation loss 2.7923\n",
      "validation accuracy 0.3114\n",
      "epoch 9, train loss: 2.7071\n",
      "validation loss 2.7701\n",
      "validation accuracy 0.3575\n",
      "epoch 10, train loss: 2.6852\n",
      "validation loss 2.7448\n",
      "validation accuracy 0.3842\n",
      "epoch 11, train loss: 2.6576\n",
      "validation loss 2.7144\n",
      "validation accuracy 0.4014\n",
      "epoch 12, train loss: 2.6231\n",
      "validation loss 2.6778\n",
      "validation accuracy 0.4115\n",
      "epoch 13, train loss: 2.5821\n",
      "validation loss 2.6368\n",
      "validation accuracy 0.4154\n",
      "epoch 14, train loss: 2.5382\n",
      "validation loss 2.5957\n",
      "validation accuracy 0.4191\n",
      "epoch 15, train loss: 2.4971\n",
      "validation loss 2.5600\n",
      "validation accuracy 0.4219\n",
      "epoch 16, train loss: 2.4640\n",
      "validation loss 2.5337\n",
      "validation accuracy 0.4243\n",
      "epoch 17, train loss: 2.4417\n",
      "validation loss 2.5181\n",
      "validation accuracy 0.4258\n",
      "epoch 18, train loss: 2.4302\n",
      "validation loss 2.5117\n",
      "validation accuracy 0.4259\n",
      "epoch 19, train loss: 2.4267\n",
      "validation loss 2.5108\n",
      "validation accuracy 0.4259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 20\n",
    "rnn3 = LMModel5(len(vocab), 64, 2)\n",
    "optimizer = torch.optim.SGD(rnn3.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.01, steps_per_epoch=len(dls_train), epochs=epochs\n",
    ")\n",
    "\n",
    "train(rnn3, epochs, dls_train, dls_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e85d4f",
   "metadata": {},
   "source": [
    "# References:\n",
    "1. Colah, Understanding LSTM Networks: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
