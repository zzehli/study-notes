{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c15a4ef",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (CNN) is a type of neural network commonly used in computer vision. CNN uses convolutions layers to detect image features. Within a convolution layer, a **kernel**/**filter** is applied to an image, which produces a **feature map**/**activation map**. Next, use **pooling** to scale down the activation map. Two common techniques of pooling are **max pooling** and **average pooling**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b482fef3",
   "metadata": {},
   "source": [
    "Here we start with a reduced MNIST dataset to classify digit 3 and 7. Then we will expand to all ten digits towards the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e239a",
   "metadata": {},
   "source": [
    "# Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96c8d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "032073be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://s3.amazonaws.com/fast-ai-sample/mnist_sample.tgz\" -O \"../data/mnist_sample.tgz\" && tar -xzf \"../data/mnist_sample.tgz\" -C ../data/\n",
    "# !wget \"https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz\" -O \"../data/mnist_png.tgz\" && tar -xzf \"../data/mnist_png.tgz\" -C ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55f2e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('../data/mnist_sample/labels.csv'), PosixPath('../data/mnist_sample/valid'), PosixPath('../data/mnist_sample/train')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "sample_path = Path(\"../data/mnist_sample\")\n",
    "print(list(sample_path.iterdir()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70e0f70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APLPCPw38R+NrWe60i3hNtBJ5byzShAGxnHqeMfmK3pfgZ4sR2SK40e4fnYsV6Mv9AQP1xXB6xouo+H9Tl07VbV7W7ixvicgkZ5HTg1Qp6zSohRJHVSckBiATXsXwU8M+EbmaLXdZ1q0/tK3n/0bT5Zlj2MCCrnJBY5zgDisj4t+FPGsGuXXiLxBDFPayvsS4tG3RRLnCJjAKjnHI5PcmvMaltoftN3DAZEiEjqnmP8AdXJxk47CvTNc+B3ie21xLTRbRtQsGSPZfmWNUclQWbGcqoJI78DPeu48Y6ivgH4Mt4R1bWE1PXrpDEEDl/LQsGOc8hQvAzjJxgY6fO1FaMGv6zawiG31a/iiAChI7l1XA6DANUJJHlkMkjs7tyWY5Jptf//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABC0lEQVR4AWNgGGSAEegenvAfxrzRB54CmS82nkFzYNc/BPhzqUoRLg3SeUeJgeHtJSDjprqAIQOD71a4LJCh7KWsLAkR4H3w799MZDlkdtS/f99NkQUQbLYZ3/79AxqMDTjN+/fvZzoHNikGs99AR//wYsUq2QPx0KlaXSzSVltfQaT/9IhhkZYz8pz9F6RgPxMWWaBQ9AmQbBl2SQaWA0DJWWBJTP1/zgIlbqFKStaFQYxi1mdg+HMSwoaSEhf/8YOZ4p1AUy+iyDGs+PfPgJOBgbP+479//z/Zo0qmAtWf3bfvLJD698kZVY5BcRlIGAR+dZnD5ECRDQbsgU63/BhuMOy7eR4mRCsaAMoAf9ycBq4UAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "im3 = Image.open(sample_path / \"train\" / \"3\" / \"12.png\")\n",
    "im3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08ce7f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0],\n",
       "        [ 91, 142, 155, 246, 182],\n",
       "        [254, 254, 254, 254, 254],\n",
       "        [254, 254, 235, 189, 189],\n",
       "        [ 35,  35,  25,   0,   0],\n",
       "        [  0,   0,   0,   0,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "im3_t = torch.as_tensor(array(im3))\n",
    "im3_t[3:10, 5:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ea803",
   "metadata": {},
   "source": [
    "We define a 3 x 3 matrix, it serves as a **kernel** in CNN. It is responsible for detecting image features. We define a kernel that detect top edge of an object. To apply on kernel on an image, we perform **element-wise multiplication** (`*` in pytorch). This kernel has -1s on the top row and 1s on third row. What this says is if the image has greater values on the bottom than top, we get a large positive number after we apply the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1535108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_edge = torch.tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bee670",
   "metadata": {},
   "source": [
    "We pick the part of the image that has a top edge, boundary of the figure 3, and apply the kernel. We get a large number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1e0f928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(762.)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(im3_t[4:7, 6:9] * top_edge).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e70cb",
   "metadata": {},
   "source": [
    "We will simplify that process with a func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "553d12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(row, col, kernel):\n",
    "    return (im3_t[row - 1 : row + 2, col - 1 : col + 2] * kernel).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c2cc0",
   "metadata": {},
   "source": [
    "To apply a kernel on an image, we swipe it scans an image one position at a time. We use nested list comprehension to call `apply_kernel` iteratively. This will give all all top edges of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47da161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_image(im, title=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Show a PIL, NumPy array, or PyTorch tensor image on matplotlib axes.\n",
    "\n",
    "    Args:\n",
    "        im: Image as PIL.Image, numpy.ndarray, or torch.Tensor\n",
    "        title: string title for the image (optional)\n",
    "        **kwargs: additional arguments passed to ax.imshow()\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle PyTorch tensors\n",
    "    if hasattr(im, \"data\") and hasattr(im, \"cpu\") and hasattr(im, \"permute\"):\n",
    "        im = im.data.cpu()\n",
    "        # Convert to numpy\n",
    "        if hasattr(im, \"numpy\"):\n",
    "            im = im.numpy()\n",
    "        else:\n",
    "            im = np.array(im)\n",
    "        # Handle channel-first format (C, H, W) -> (H, W, C)\n",
    "        if (\n",
    "            len(im.shape) == 3 and im.shape[0] < 5\n",
    "        ):  # Assume channels first if first dim < 5\n",
    "            im = np.transpose(im, (1, 2, 0))\n",
    "\n",
    "    # Handle single channel images (squeeze last dimension if it's 1)\n",
    "    if len(im.shape) == 3 and im.shape[-1] == 1:\n",
    "        im = im.squeeze(-1)\n",
    "\n",
    "    # Calculate default figure size based on image dimensions\n",
    "    height, width = im.shape[:2]\n",
    "    max_size = 1\n",
    "    if height > width:\n",
    "        figsize = (max_size * width / height, max_size)\n",
    "    else:\n",
    "        figsize = (max_size, max_size * height / width)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    if \"cmap\" not in kwargs:\n",
    "        kwargs[\"cmap\"] = \"Grays\"\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im, **kwargs)\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    # Set title if provided\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c295b2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = range(1, 27)\n",
    "top_edge3 = torch.tensor([[apply_kernel(i, j, top_edge) for j in rng] for i in rng])\n",
    "\n",
    "show_image(top_edge3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029a390",
   "metadata": {},
   "source": [
    "Here, we can see the top boundaries of the figure 3 are darker, since the kernel has identified those parts of the image as top edge.\n",
    "\n",
    "As another example, we define a left edge kernel and apply it to the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b008519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_edge = torch.tensor([[-1, 1, 0], [-1, 1, 0], [-1, 1, 0]]).float()\n",
    "\n",
    "left_edge3_tensor = torch.tensor(\n",
    "    [[apply_kernel(i, j, left_edge) for j in rng] for i in rng]\n",
    ")\n",
    "\n",
    "show_image(left_edge3_tensor);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d25071",
   "metadata": {},
   "source": [
    "# Create a convolution in Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a52243",
   "metadata": {},
   "source": [
    "Now we use pytorch to create a CNN: `F.conv2d(input, weight)`, where input is the input tensor of shape `(minibatch, in_channels, iH, iW)` and weight is filters of shape `(out_channels, in_channels, kH, kW)`. **Channel** is the number of basic color to represent an image. In our case, we use greyscale for training images, which means our input channel is 1. We will create 4 kernels, so we can detect 4 features, therefore our output channel is 4. Note that pytorch represent images with dimension `(channels, height, width)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ce37992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag1_edge = torch.tensor([[0, -1, 1], [-1, 1, 0], [1, 0, 0]]).float()\n",
    "diag2_edge = torch.tensor([[1, -1, 0], [0, 1, -1], [0, 0, 1]]).float()\n",
    "\n",
    "edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])\n",
    "edge_kernels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a0b72",
   "metadata": {},
   "source": [
    "Notice that the kernel shape has rank 3, missing the `in_channels` param. We use `unsqueeze` to add a new dimension of size 1 at position 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bcc7508a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_kernels.shape, edge_kernels.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "057bab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_kernels = edge_kernels.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23394fb1",
   "metadata": {},
   "source": [
    "Next, we create dataloaders for the images with minibatch size of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98c5b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading MNIST images from directory structure\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        # Get all image files\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            if class_dir.is_dir():\n",
    "                for img_file in class_dir.glob(\"*.png\"):\n",
    "                    self.images.append(str(img_file))\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image as PIL Image (grayscale)\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def create_mnist_dataloaders(path, batch_size=64):\n",
    "    path = Path(path)\n",
    "\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),  # Convert PIL to tensor and normalize to [0, 1]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Check if train and valid directories exist\n",
    "    train_path = path / \"train\" if (path / \"train\").exists() else path / \"training\"\n",
    "    valid_path = path / \"valid\" if (path / \"valid\").exists() else path / \"testing\"\n",
    "\n",
    "    if train_path.exists() and valid_path.exists():\n",
    "        train_dataset = MNISTDataset(train_path, transform=transform)\n",
    "        print(train_dataset.class_to_idx)\n",
    "        print(\"train set len, \", len(train_dataset.images))\n",
    "        valid_dataset = MNISTDataset(valid_path, transform=transform)\n",
    "        print(valid_dataset.class_to_idx)\n",
    "        print(\"valid set len, \", len(valid_dataset.images))\n",
    "\n",
    "    else:\n",
    "        print(\"Using existing train/valid split\")\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb440df5",
   "metadata": {},
   "source": [
    "Now take a look at the input dataset's shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6af1a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3': 0, '7': 1}\n",
      "train set len,  12396\n",
      "{'3': 0, '7': 1}\n",
      "valid set len,  2038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, valid_loader = create_mnist_dataloaders(\"../data/mnist_sample\")\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60fac2",
   "metadata": {},
   "source": [
    "Now we are ready to create a convolutional layer. Notice that it gives use individuals images of shape (4, 26, 26), where each image has 4 out channels (4 edges). Notice the output image has height and width of 26, this is because the filters cannot give use the same shape without padding on the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50710fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 26, 26])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_features = F.conv2d(xb, edge_kernels)\n",
    "batch_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b19b3872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(batch_features[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb252e",
   "metadata": {},
   "source": [
    "# Math of the convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303c746",
   "metadata": {},
   "source": [
    "A crucial aspect of CNN is to figure out the shape of the **activation map** (output of the convolution) after a kernel is applied. For example, for a 5 x 5 image, applying a 3 x 3 kernel gives a 2 x 2 activation map. As shown in the gif, a 6 x 6 image turns into a 4 x 4 activation map after applying a 3 x 3 kernel.\n",
    "\n",
    "TODO: image\n",
    "\n",
    "Oftentimes, we want the activation map the same size as the input image. To do that we need to add **padding** grids on each side of the input image. If the kernel size is `ks` by `ks`, we need add `ks//2` padding on each side to keep the activation map the same shape. In our previous example, our kenels are 3 x 3, we need `3 // 2 = 1` padding on each side of the original image to keep the same dimension.\n",
    "\n",
    "To reduce the size of the activation map, we use **stride**, which means we move the kernel in steps of n instead of 1 grid at a time. When we take stripe of 2, we reduce the size of one side by 1/2, which means the activation map is 1/4 the original size.\n",
    "\n",
    "In general, the formula for each dimension is `(n + 2*pad - ks)//stride + 1`, where `n` is the size of the original image on that dimension, `pad` is the padding, `ks`, the size of our kernel, and `stride` is the stride. For example, with a 5×5 input, 4×4 kernel, and 2 pixels of padding, we have `(5 + 2 * 2 - 4)//1 + 1 = 5//1 + 1 = 6`, as shown in the image below TODO: image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03fb43",
   "metadata": {},
   "source": [
    "# Create a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b346d15",
   "metadata": {},
   "source": [
    "In Pytorch, a convolution layer is `nn.Conv2d`, where it takes  `(in_channels, out_channels, kernel_size)` for required args. It initializes weight matrix automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "919c58df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_cnn = nn.Sequential(\n",
    "    nn.Conv2d(1, 30, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(30, 1, kernel_size=3, padding=1),\n",
    ")\n",
    "broken_cnn(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00def13b",
   "metadata": {},
   "source": [
    "This CNN won't be able to classify our digits, since the final activation is 28 x 28. To perform classification, we need a single output with 2 levels (3 and 7). To achieve that, we use `stride=2` to reduce the size of the activation maps. We create a helper function to create convolution layer as well as an optional activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1fabdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(ni, nf, ks=3, act=True):\n",
    "    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks // 2)\n",
    "    if act:\n",
    "        res = nn.Sequential(res, nn.ReLU())\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914f280",
   "metadata": {},
   "source": [
    "The stride 2 is baked into the `conv` definition. For our cnn, we add more layers to compensate for the fact that the activation maps are getting smaller. Notice the `Flatten` function removes the extra 1 x 1 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c2104fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = nn.Sequential(\n",
    "    conv(1, 4),  # 14x14\n",
    "    conv(4, 8),  # 7x7\n",
    "    conv(8, 16),  # 4x4\n",
    "    conv(16, 32),  # 2x2\n",
    "    conv(32, 2, act=False),  # 1x1\n",
    "    nn.Flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfa11d",
   "metadata": {},
   "source": [
    "Below is the a summary from the fastai learner's summary() function:\n",
    "\n",
    "```\n",
    "Sequential (Input shape: ['64 x 1 x 28 x 28'])\n",
    "================================================================\n",
    "Layer (type)         Output Shape         Param #    Trainable \n",
    "================================================================\n",
    "Conv2d               64 x 4 x 14 x 14     40         True      \n",
    "________________________________________________________________\n",
    "ReLU                 64 x 4 x 14 x 14     0          False     \n",
    "________________________________________________________________\n",
    "Conv2d               64 x 8 x 7 x 7       296        True      \n",
    "________________________________________________________________\n",
    "ReLU                 64 x 8 x 7 x 7       0          False     \n",
    "________________________________________________________________\n",
    "Conv2d               64 x 16 x 4 x 4      1,168      True      \n",
    "________________________________________________________________\n",
    "ReLU                 64 x 16 x 4 x 4      0          False     \n",
    "________________________________________________________________\n",
    "Conv2d               64 x 32 x 2 x 2      4,640      True      \n",
    "________________________________________________________________\n",
    "ReLU                 64 x 32 x 2 x 2      0          False     \n",
    "________________________________________________________________\n",
    "Conv2d               64 x 2 x 1 x 1       578        True      \n",
    "________________________________________________________________\n",
    "Flatten              64 x 2               0          False     \n",
    "________________________________________________________________\n",
    "\n",
    "Total params: 6,722\n",
    "Total trainable params: 6,722\n",
    "Total non-trainable params: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f687e0",
   "metadata": {},
   "source": [
    "We interpret the shape as `batch,channel,height,width`. Notice that the # of parameters are increasing, this compensate the fact that the activation maps are getting smaller. Let's see why that's needed.\n",
    "\n",
    "We look at the first layer of this cnn, the output shape is 14 x 14. The next layer has 296 parameters. These weights include n_channel = n_bias bias weights. In this case, we 8 bias weights. So each activation in the map will multiply 296 - 8 parameters, which is 14 * 14 * 288 = 56448 operations\n",
    "\n",
    "For the next layer, we have 1169 - 16 params as well as 4 x 4 activation maps. This gives us 1153 * 4 * 4 = 56448 operations, which equals the operation we perform in the previous layer.\n",
    "\n",
    "So although we decreased the output dimension, we does not decrease the amount of computation we perform. Since later layers computer more complex features, this is the desired outcome. As a rule of thumb, we generally want to double the number of filters each time we have a stride-2 layer.\n",
    "\n",
    "> Note that the terms here are a big confusing. Sometimes channels are called features or filters when they are not input channels. TODO: reformat in blog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f643176",
   "metadata": {},
   "source": [
    "Below is how to count the total params of a layer, parameters of its weight and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be6cd981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count for simple_cnn[0]: 40\n",
      "36\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "m = simple_cnn[0]\n",
    "out = m(xb)  # xb is your input batch\n",
    "param_count = sum(p.numel() for p in m.parameters())\n",
    "print(f\"Parameter count for simple_cnn[0]: {param_count}\")\n",
    "for p in m.parameters():\n",
    "    print(p.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3d2f7",
   "metadata": {},
   "source": [
    "Finally, we apply the cnn to a training batch and look at its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f837edd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 2]),\n",
       " tensor([[0.0364, 0.0341],\n",
       "         [0.0370, 0.0353],\n",
       "         [0.0365, 0.0352],\n",
       "         [0.0371, 0.0348],\n",
       "         [0.0371, 0.0330],\n",
       "         [0.0364, 0.0356],\n",
       "         [0.0379, 0.0349],\n",
       "         [0.0356, 0.0335],\n",
       "         [0.0360, 0.0343],\n",
       "         [0.0342, 0.0342]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_cnn(xb).shape, simple_cnn(xb)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e1692",
   "metadata": {},
   "source": [
    "This is exactly what we want, for each of the 64 images in a batch, we got two scores, logits, each corresponds to a digit.\n",
    "\n",
    "We pick an image from the training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16f011be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb.shape, show_image(xb[3, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdec19",
   "metadata": {},
   "source": [
    "We convert the logits to probability and we can see that the classification with higher probability is 0, which according to our dataset (`class_to_idx`) above, is 3. This is an incorrect classification. We will see if the training improves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f024a0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0371, 0.0348], grad_fn=<SelectBackward0>),\n",
       " tensor([0.5006, 0.4994], grad_fn=<SelectBackward0>),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = simple_cnn(xb)  # [64, 2]\n",
    "probs = logits.softmax(dim=1)  # [64, 2], rows sum to 1\n",
    "preds = probs.argmax(dim=1)\n",
    "logits[3], probs[3], preds[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ed9dd",
   "metadata": {},
   "source": [
    "Now we create a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1a30825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss: 0.6928\n",
      "validation loss tensor(0.6924)\n",
      "epoch 1, train loss: 0.6901\n",
      "validation loss tensor(0.6858)\n",
      "epoch 2, train loss: 0.4942\n",
      "validation loss tensor(0.1410)\n",
      "epoch 3, train loss: 0.0925\n",
      "validation loss tensor(0.0802)\n"
     ]
    }
   ],
   "source": [
    "def train(model, lr, epochs, train_loader, valid_loader):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_num = 0\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            pred = model(xb)\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_num += 1  # number of batches within a epoch\n",
    "        avg_loss = epoch_loss / batch_num\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_num_valid = 0\n",
    "            valid_loss = 0\n",
    "            for xb, yb in valid_loader:\n",
    "                valid_loss += F.cross_entropy(model(xb), yb)\n",
    "                batch_num_valid += 1\n",
    "        print(f\"epoch {epoch}, train loss: {avg_loss:.4f}\")\n",
    "        print(\"validation loss\", valid_loss / batch_num_valid)\n",
    "\n",
    "\n",
    "train(simple_cnn, 5e-2, 4, train_loader, valid_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f4ce1",
   "metadata": {},
   "source": [
    "now we've trained the CNN, we run our example again. We got a different classification. Now it correctly classify the digit as 7. And this time the model is a lot more confident, with 7 getting a probability of 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "687e7828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.1781,  1.0745], grad_fn=<SelectBackward0>),\n",
       " tensor([0.0951, 0.9049], grad_fn=<SelectBackward0>),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = simple_cnn(xb)  # [64, 2]\n",
    "probs = logits.softmax(dim=1)  # [64, 2], rows sum to 1\n",
    "preds = probs.argmax(dim=1)\n",
    "logits[3], probs[3], preds[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591bb799",
   "metadata": {},
   "source": [
    "Now we a ready to train the full 10 digits. Notice here we doubled the filters of every layer to increase the number of parameters.\n",
    "\n",
    "Previously, our first layer had 4 output filters. That meant that there were 4 values being computed from 9 pixels (3 x 3 kernel) at each location. Think about what happens if we double this output to 8 filters. Then when we apply our kernel we will be using 9 pixels to calculate 8 numbers. That means it isn't really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they're forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs.\n",
    "\n",
    "To fix this, we can use a larger kernel in the first layer. If we use a kernel of 5×5 pixels then there are 25 pixels being used at each kernel application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c1cee7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n",
      "train set len,  60000\n",
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n",
      "valid set len,  10000\n",
      "epoch 0, train loss: 1.3969\n",
      "validation loss tensor(0.2357)\n",
      "epoch 1, train loss: 0.1867\n",
      "validation loss tensor(0.1531)\n",
      "epoch 2, train loss: 0.1208\n",
      "validation loss tensor(0.0973)\n",
      "epoch 3, train loss: 0.0924\n",
      "validation loss tensor(0.1001)\n",
      "epoch 4, train loss: 0.0767\n",
      "validation loss tensor(0.0702)\n"
     ]
    }
   ],
   "source": [
    "full_train_loader, full_valid_loader = create_mnist_dataloaders(\"../data/mnist_png\")\n",
    "larger_cnn = nn.Sequential(\n",
    "    conv(1, 8, ks=5),  # 14x14\n",
    "    conv(8, 16),  # 7x7\n",
    "    conv(16, 32),  # 4x4\n",
    "    conv(32, 64),  # 2x2\n",
    "    conv(64, 10, act=False),  # 1x1\n",
    "    nn.Flatten(),\n",
    ")\n",
    "\n",
    "train(larger_cnn, 5e-2, 5, full_train_loader, full_valid_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1659b9",
   "metadata": {},
   "source": [
    "Let's look at one sample image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44517be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAABmlJREFUeJztnUtv01oXht/tWy527gk0DaoDjUQHCCbABP4Bf5Z/wBgJMUCCjiLSqI0KThPiBCe+bO8zOfYp7YGmPf2+LKf7kTKp4mo3j9e+rLWcMiGEgGSjKJsegERKIIGUQAApgQBSAgGkBAJICQSQEgggJRBASiCAlEAAKYEAUgIBpAQCSAkEkBIIICUQQEoggJRAACmBAFICAbRND2Bd1mkK+d17GGPX+vn/m8xIOE8cx+Ccg3MOx3EwHo+xWq3gOA5ms9kv77137x5s20ahUEClUkGlUkk/fCEECRGZlMA5h+/78H0fh4eH+PjxI6bTKT58+IB+v59GBGMML168wJs3b9BqtdDr9WBZFjSN1p9NazR/QAgBzjmEEFitVlgsFlitVphMJnAcB2dnZzg5OcFwOEyvYYyh0+lgPB5D0zR4noc4jslEQEJmJCyXS4xGI/z8+RODwQCfPn2C67o4Pj7G8fExlsslptPppQ93NBrh3bt3qNVqYIzBtm3k83kYhkEmImiMYg08z8NgMIDjOHj//j3evn2LyWQCznl6d3POL0k4OTnBt2/fYFkWut0uXr9+DSEEVFWVEq4L5xzL5RKLxQKLxQKe58HzvCuvi+MYQRDA931EUQQhRPqiQmYk+L6P0WiE4XCI8XiMKIo2PaRbIzOHtSAIMJ1O8f37d7iuC875pod0a2RGgq7rqFQqaDQasCwLqqpueki3Rmamo2KxiP39fZTLZbiuC13XNz2kWyMzEnRdR7lcBucclmVBUTITxFeSGQmqqqJUKkEIgWKxKCVsAsMwUK/X0xzQNq0JmbmdGGPQNA26rv9nAZRSFkCGIuGmKIoCTdOQy+Wg6zoURQFjjJSIzETCTVFVFYZhpLkiRVFSEVTYegmapqFYLMI0TeRyOaiqSk7C1k9H1WoVvV4PtVoNnU4HpmmiUCiQWti3XoJpmtjd3UWj0UC9Xkc+n08PelSiITMShBCIoghhGF47b3RxIaby4SdkZk3gnGO1WsHzPPi+jziONz2kWyMzEm4aCdTu+n8jM9PRcrnEcDiE4zg4PT1FGIZXXsMYg2VZsG0brVYrLXFSIzMS5vM5Pn/+jK9fv6Lf7yMIgrWuazQaePLkCdrtNtrtNqldUUJmpiPOOTzPS7ss1l0TNE2DaZqwLAuGYfyPR3kzMiPhpui6DsuyUCqVkMvlSE5Hd0aCjIQNk5wTKEYBcEckUEdKIADpLaoQIu2u830fs9kMk8kEi8Xij7sjxhhUVYWqqmkNQVVVstMReQlBECAMQ0ynU/T7fXz58gWu6/7xsKaqKkzThGEYaQo7qchRFEFeAuccURT9EglX5Y4URYGu62khh2IN4TykJQD/TEmc80s9pb8jiYSkmHNeBEXIS0gEhGGYZlGvauhNuvUqlQrK5fIv9WWKkJZwPgqS1zrpCsYYdF2HrutpXZnyOYG0hDiO4XkeZrMZFovF2insZE04HwGU1wSa8fk3FyWs2w6fRMLFNheqkJaQPJ82n8/hed7akcAYS+/+81MRVRGkp6MgCDAcDnF4eIjBYLDWkznA5UigvkUlHQlRFMFxHBwdHeH09BS+7691XdIymYUzAkA8EhJu4xkzyhJIR8JdQUoggJRAACmBAFspIWkUi6IoTXUkdQlKD5EnZGJ3dF2SRrHJZIK9vT3MZrP0+ywoFvu3VsLR0RE0TcPBwQFc10WpVEoPcdS2q1spIakn5HI5FIvFtKJGNXWxlRLq9TpevnyJ+/fv4/nz52g2m7Asi+wD6FspwbIsHBwc4NGjR9jf30epVEI+nycZBQBxCZqmodVqodvtQlEUVKvVNKWdNARblpWWMAuFAgzDgG3bePDgAXZ2djLxzDNpCYVCAc+ePcPDhw/TTGocx3BdF2dnZwCAvb09PH78GKZpwrZtNJtNNJtNPH36FI1GA4VCgWwPagJpCZqmodlsolqtYjaboVqtwjRNBEGQ3t3lchk7Ozsol8vo9XrY3d1FtVpFp9NBpVJJf5eUcEOSJi4AaLVaePXqFWzbhud5+PHjBwCg2+2mX7nZbrdRq9VQLBZJngd+B6P8r4CToQkhEIYh5vM5wjBMi/8A0gNYUldOtqMXzwOUI4G0hLvCVuaOsoaUQAApgQBSAgGkBAJICQSQEgggJRBASiCAlEAAKYEAUgIBpAQCSAkEkBIIICUQ4C/xx6TyPnyIZgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(full_train_loader))\n",
    "show_image(xb[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6fe642",
   "metadata": {},
   "source": [
    "Use our trained model to classify this image, the prediction is 1, which is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b730186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-3.2632, 10.2683,  0.5308,  1.6681, -2.3475, -3.7332, -6.7423, -0.2727,\n",
       "          4.8563, -0.7971], grad_fn=<SelectBackward0>),\n",
       " tensor([1.3222e-06, 9.9527e-01, 5.8747e-05, 1.8320e-04, 3.3034e-06, 8.2634e-07,\n",
       "         4.0768e-08, 2.6305e-05, 4.4417e-03, 1.5571e-05],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = larger_cnn(xb)\n",
    "probs = logits.softmax(dim=1)\n",
    "preds = probs.argmax(dim=1)\n",
    "logits[0], probs[0], preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335132d8",
   "metadata": {},
   "source": [
    "# BatchNorm\n",
    "\n",
    "Our results from the initial training is relatively good. We can improve the results by applying **Batch Normalization** to our convolution layer. The problem with convolutions is that nearly all activations start at 0. This will affect the training results. We want to apply normalization to the activations to improve the training results.\n",
    "\n",
    "Batch normalization works by taking an average of the mean and standard deviations of the activations of a layer and using those to normalize the activations. We also add two learnable parameters `gamma` and `beta` to accommodate extreme values. The new activation after applying batchnorm is `gamma*y + beta` where `y` is normalized activation. In pytorch, we use `nn.BatchNorm2d()` to apply batchnorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6ac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss: 0.2530\n",
      "validation loss tensor(0.0933)\n",
      "epoch 1, train loss: 0.0963\n",
      "validation loss tensor(0.0649)\n",
      "epoch 2, train loss: 0.0689\n",
      "validation loss tensor(0.0540)\n",
      "epoch 3, train loss: 0.0543\n",
      "validation loss tensor(0.0429)\n",
      "epoch 4, train loss: 0.0459\n",
      "validation loss tensor(0.0395)\n"
     ]
    }
   ],
   "source": [
    "def conv(ni, nf, ks=3, act=True):\n",
    "    layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks // 2)]\n",
    "    if act:\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.BatchNorm2d(nf))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cnn_bn = nn.Sequential(\n",
    "    conv(1, 8, ks=5),  # 14x14\n",
    "    conv(8, 16),  # 7x7\n",
    "    conv(16, 32),  # 4x4\n",
    "    conv(32, 64),  # 2x2\n",
    "    conv(64, 10, act=False),  # 1x1\n",
    "    nn.Flatten(),\n",
    ")\n",
    "\n",
    "train(cnn_bn, 5e-2, 5, full_train_loader, full_valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce060b7",
   "metadata": {},
   "source": [
    "We can see that after applying the batch norm, the training loss goes down even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
